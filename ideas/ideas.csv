quote,name,date
"How to scale a collections as data up and down? How to accommodate top-of-system R1 universities with professional schools, I-schools, etc., and middle-tier, lower-tier, community college, small liberal arts colleges with fewer resources and expertise.","Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
"Can there be a network approach for access and discovery? A registry of datasets/collections ready to use (with metadata, context that gives users a sense of possible uses, level of difficulty)? (Dutch example of using FAIR principles to score datasets). (Alternative: give guidance about uses and difficulty of datasets by providing usage precedents--e.g., which has been used for courses, etc.)","Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
"Can the conceptual assumptions in the library world about what is a  collection,   archive,   series,  etc. --and then what is a  dataset  -- be made transparent as the preliminary context that users need to understand what they can do with collections as data?","Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
"Can we expose/extract  data primitives  (named entities, geographical references, dates, etc.) that would function as a kind of lingua franca for datasets--allowing an initial way for researchers to use data in their own contexts and projects?","Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
Could iPython/Jupyter notebooks be the natural finding aid format for datasets in the future (allowing for description and also custom harvesting/searching/scraping/analysis)?,"Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
How do we deal with the outsize influence of the proprietary databases/datasets in this space?,"Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
"Instead of thinking of  collections/datasets, perhaps think of providing unique IDs for granular data, and constructing datasets on the fly after the fact?","Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
We need a logic for where people deposit datasets - in institutional repository? In a journal's Dataverse? In a national or international repository?,"Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
"Can we plan for a future in which it is not datasets but Dockerized containers that are deposited in repositories (i.e., entire computing and research workflows, including datasets)?","Greg Jansen, Lisa Johnston, Matthew Miller, Alan Liu, Mia Ridge",
How can computational research services create better pathways to interpretation through tools and methods for the smooth traversal between reduction and abstraction inherent in derivation and aggregation?,Jefferson Bailey,
How can new access models help researchers have greater comfort with technical mediation at multiple levels and with an increasing distance between the granularity and totality of the object(s) of study?,Jefferson Bailey,
"How can programs address the challenges still inherent, even with derived datasets, of limited technical proficiency and local infrastructure?",Jefferson Bailey,
Recent empirical research has confirmed that digital tools and technologies are fundamentally changing how scholars work. Yet the inverse of this relationship has received little attention how is infrastructure changing to support emergent scholarly practice?,Alexandra Chassanoff,
"How might we envision ways to access born-digital materials? Relatedly, how might we use born-digital materials in our research? What kinds of questions could be asked and answered from examination of contents of the so-called black box? ",Alexandra Chassanoff,
To what extent will the guidelines that we generate during Always Already Computational take digitization workflows into account? Can we advise libraries and archives on how an understanding of an eventual data framework can be integrated into these workflows such that when requests for funding are made our colleagues can anticipate generating the kinds of data that we will need for a data access environment?,Tanya Clement,
"How do we account for (new, collective) data collection that accounts for haunting imprints and outright absences in the archives upon which we depend? ","Gabrielle Foreman, Labanya Mookerjee",
"Not all of the data we create or purchase for Library collections comes in neat multi-gigabyte packages of ordered files: We recently discovered that datasets we had purchased as part of a database licensing negotiation were more shelf ready than machine ready: They currently exist as stacks of hard drives, discs, and other bewildering formats sitting on a book cart. How do we provide access to these data collections?",Harriett Green,
". . . I would ask, how can we conceptualize the full spectrum of data usability? It is not enough for us to digitize the collection materials and for the data to exist on someone's server:  Usability encompasses data formats, tool interoperability to the negotiated permissions and rights for researchers to share and manipulate data as they engage in analytic workflows. ",Harriett Green,
"If library collections, including but not limited to that of digital repository platforms, are considered (primarily digital repositories are targeted in the proposal), there is a wealth of data and metadata (*data) that already exists. Better yet, memory institutions already work with this *data at scale using traditional and emerging technologies that underpin and are hidden by delivery and discovery interfaces. How can this underlying ecosystem be better leveraged for computational data analysis by researchers? i.e. do we just need to make access to a Solr index publicly available? Can we plug into our library data ETL systems a public Hadoop integration point? Do we need to better document and expose to new communities our existing data APIs or data exchange protocols? ",Christina Harlow,
" . . . what is the responsibility of the originating memory institution to support capture of that computational data output for sake of archiving, reproducibility, discoverability, and expanded *data services?",Christina Harlow,
"Capacious collections data must remember enough and forget enough to be useful. For which terms will we expend the effort to do this reconciliation? Which edge cases will we try to capture in an ever-more-complex data model? Opinions on how to draw that line will frequently set the librarian, the historian, and the machine at cross purposes. Outlining the necessary competencies a collections data production team needs, and the key questions, in order to navigate perspectives must therefore be a crucial output of this forum.",Matthew Lincoln,
"The most potentially kinetic computationally amenable data comes from the conversion and processing of documents themselves. Transforming documents into data at the New York Public Library took the form of small projects that converted special collection materials into datasets through the power of algorithms, staff and the crowd. The results were a domain specific dataset often with a necessarily unique data model. Taking stock of the growing number these datasets we theorized about their possible integration with our traditional metadata systems. Would it be possible to go beyond simply linking to the dataset as a digital asset? If we were to build a RDF metadata system from the ground up could we begin thinking of it as an open-world assumption system where the contents of these datasets could exist alongside traditional bibliographic metadata? ",Matthew Miller,
". . . when thinking about these resources as discrete datasets, what work could be done to improve their use and interoperability? WC3 standards such the VoID Vocabulary provided the means to describe the metadata about datasets. Leveraging such standards and establishing best practices and preferred authorities could we increase access across humanities datasets?",Matthew Miller,
"The topic of preprocessing introduces the question of best practices and standards that could be followed to ensure the broadest access to our datasets. What are some additional use cases that could drive shared best practices or tools for releasing cultural heritage data? Are there more advanced preprocessing that could be done to some of the common archetypical data formats found in libraries, archives and museums? And what sort of resources are required in an organization to process datasets for public consumption? ",Matthew Miller,
Are researchers genuinely invited to engage with library collections as data?,Anna Neatrour,
"What does a digital collections as data repository look like? Providing additional layers and portals that leverage computational exploration to existing collections might serve as an intermediate step. Imagine if text based digital collections also had a Voyant-like layer built into the digital repository itself that researchers could use, along with pre populated queries and visualizations so people at the beginning stages of inquiry could see examples of text analysis.",Anna Neatrour,
"Could our digital repositories provide a mechanism for researchers to curate their own research collections, providing a space where digital library objects could be combined with researcher supplied data?",Anna Neatrour,
"Data without context is meaningless. Data with context but without social awareness is deceptively meaningless. With that deception comes, in the worst case, the use and articulation of argument founded on a lack of understanding and awareness of perpetuating ideas that are intrinsically linked to the creation and curation of said data. A question for this group would be; how do we attempt to preserve that context without overwhelming the user?",Hannah Skates Kettler,
"How can we integrate generations of high-quality, professionally-created metadata with electronic versions of the object itself? Particularly when copyright comes into play, we can't simply hope for openness; and there's a steep trade-off between the thoroughness of a well-thought-out standard and a simplicity of conception that makes a digital resource useful for (for instance) a graduate student just beginning to get interested in working with large collections.",Ben Schmidt,
"The technical demands of even downloading something like the HTRC EF set exceed both the technical competencies and computing infrastructure of most humanists--I've literally spent several weeks recently, restarting downloads and identifying missing files as I try to fill up a RAID array with several terabytes of data. Processing these files into the raw material of research is even harder. So how do we make collections accessible for work?",Ben Schmidt,
"Expand accessibility of data - (a) creating middleware tools: APIs or a model like Trove Harvester, which enables people build their own API queries and ways to get content
(b) Rights statements and access - false claims of copyright, generic statements by libraries that shouldn't be applied
(c) Workflows for remediation for legacy items in the digital collection.","John Ajao, Jefferson Bailey, Daniel Fowler, Harriett Green, Anna Neatrour, Tim Sherratt ",
"Improve discoverability of data - (a) How can we make the datasets in Github (e.g., Carnegie Museum of Art, https://github.com/cmoa ) discoverable? (b) Alternatives to Github? - create our own, non-commercial github for cultural heritage or leverage existing initiatives (SHARE?).","John Ajao, Jefferson Bailey, Daniel Fowler, Harriett Green, Anna Neatrour, Tim Sherratt ",
"Middleware: Assuming there is some sort of access to collections via API . . . Some way of incentivizing developers, students with skills to do demos? Consulting? Leverage computer science classes for projects?  -- Input from multiple relevant people in the organization in this development process. ","John Ajao, Jefferson Bailey, Daniel Fowler, Harriett Green, Anna Neatrour, Tim Sherratt ",
"Middleware: Assuming there is some sort of access to collections via API . . . Create documentation to support and encourage innovative projects with APIs  - document processes and workflows in approach to developing new portals with APIs, going beyond traditional documentation for digitization.","John Ajao, Jefferson Bailey, Daniel Fowler, Harriett Green, Anna Neatrour, Tim Sherratt ",
"Middleware: Assuming there is some sort of access to collections via API . . . Have model programs for collaboration across disciplines that might use APIs.

","John Ajao, Jefferson Bailey, Daniel Fowler, Harriett Green, Anna Neatrour, Tim Sherratt ",
"Show different levels of how to pursue a project: One with CSV, one example with exposed API queries, one example with a Github repo and thousands of data files.","John Ajao, Jefferson Bailey, Daniel Fowler, Harriett Green, Anna Neatrour, Tim Sherratt ",
"Be willing to experiment and get stuff out there, less focus on perfection -- need support for experimentation, trial and error.","John Ajao, Jefferson Bailey, Daniel Fowler, Harriett Green, Anna Neatrour, Tim Sherratt ",
This is about people. Not just technology.,"P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
"Principles of Use Agreement (use standards) ' upon request to download, interim page before entering electronic resource (like an IRB') Establish a protocol so researchers will encounter context of the construction of the data set, and voices of people from whom the data emerged. ","P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
"Statement of ethical context - description of ethical context surrounding the collection/resource ' unique and highly contextual to each data set (for example, issues such as naming) ' or a statement about why a dataset does not pose ethical issues (could change over time).","P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
Communicate to users the ethical issues and expectations around data sets.,"P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
Who gets to decide if a collection or data set has ethical issues?,"P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
"Collaborate with existing communities/technologies/approaches (for example IIIF). The work of this national forum can inform the development and future work of established projects that share similar communities, technical infrastructure, and specifications (IIIF) IIIF is relevant to these discussions and can inform/be informed by this group - (a) Interoperability (b) JSON-LD
(c) ... AV extensions (d) ... best practices for discovery of IIIF-compliant content.
","P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
"Consider users - it's challenging to create best practices around audiences we don't know - need to definitely include acknowledgement of cultural considerations and power dynamics - naming your subjects - data set of ethics - MOU attentive to set of practices for a project - basic expectations and considerations for using data, for example citing. ","P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
Rhetoric is important for example 'citizen' vs 'community'.,"P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
Open data' - need to look at rhetoric so we're clear about hierarchies that structure our data access.,"P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
There are assumptions and problematic conclusions that we have made in the past that need to be further documented and accounted for - goes across the entire workflow. Need to identify these areas.,"P. Gabrielle Foreman, Julie Hardesty, Miriam Posner, Sheila Rabun , Santi Thompson

",
"Develop pedagogy and curriculum plans for humanities information professionals, e.g. DH Commons repository.","Tanya Clement, Christina Harlow, Ben Schmidt, David Seubert, Kate Zwaard",
Extend the concept of reference to include writing code.,"Tanya Clement, Christina Harlow, Ben Schmidt, David Seubert, Kate Zwaard",
Build coalitions with groups like the National Institute for Computer-Assisted Reporting (NICAR) that share similar goals but may not know their work is aligned.,"Tanya Clement, Christina Harlow, Ben Schmidt, David Seubert, Kate Zwaard",
Write common infrastructure guidelines for dataset ... Allows forking and versioning. Exposed through an API. Exposed through bulk download.,"Tanya Clement, Christina Harlow, Ben Schmidt, David Seubert, Kate Zwaard",
Create standards for serving 'non-raw' data. Things like tokenized features. (HTRC and JStor have different tokenization schemes. Is that ideal?). Inferred automatic metadata.,"Tanya Clement, Christina Harlow, Ben Schmidt, David Seubert, Kate Zwaard",
Identify models of ipython notebooks with containerization to protect copyright.,"Tanya Clement, Christina Harlow, Ben Schmidt, David Seubert, Kate Zwaard",
Develop professional standards for privacy and content concerns about born-digital personal data.,"Tanya Clement, Christina Harlow, Ben Schmidt, David Seubert, Kate Zwaard",
How do you do social media collection in an age of surveillance?,"Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
"Promoted models of data interaction are the more supported or highly funded projects, the 1% of projects ... What data literacy skills do users need to know upfront? What data will align with certain skill sets?","Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
"A user to consider is the person creating / curating the collection. They're not external to libraries, they're librarians. [We should] allow for reflective activities to identify gaps in data. What kinds of competencies or subject specialities ought to be embedded in library data projects?","Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
Library role to provide collections as data or follow data through entire lifecycle? When do we let go? To what extent do we support research - how involved are we in data transference? Are there primitives for paradata documentation?,"Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
What context are libraries responsible for providing when they publish collections data?,"Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
"Is there a line between what decisions and standardization the library should be doing, vs. what tasks should be left to the researcher?","Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
What do you need to check off when describing the provenance development of [collections as] data?,"Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
Where's the 'I'd like to argue with this metadata' button?',"Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
"Time to admit that libraries aren't here to work for everyone. In this event, [libraries] are 100% higher ed research institutions. ","Alexandra Chassanoff, Jen Guiliano, Hannah Skates Kettler, Matthew Lincoln, Laila Shereen Sakr, Timothy St. Onge",
"In testing multiple models internally, and surveying and collaborating with similar efforts in the community, [the Internet Archive] developed a loose typology of program models for research services . . . Cyberinfrastructure Model: A custodial/archival institution provides free/subsidized access to its own computing environment that is pre-loaded with data, VMs, and other tooling. Researchers can do analysis in this remote environment and export results.",Jefferson Bailey,
"In testing multiple models internally, and surveying and collaborating with similar efforts in the community, [the Internet Archive] developed a loose typology of program models for research services . . . Bulk Data Model: The totality of domain, global-scale crawl, or large born-digital collection is transferred to researchers via data shipped on drives. Analysis takes place locally, usually in a researcher’s own high-performance computing environment.",Jefferson Bailey,
"In testing multiple models internally, and surveying and collaborating with similar efforts in the community, [the Internet Archive] developed a loose typology of program models for research services . . . Roll Your Own Model: Researchers receive support, generally in the form of funded or sponsored services, to create their own tools and leverage existing data platforms for candidate collection building and analysis.",Jefferson Bailey,
"In testing multiple models internally, and surveying and collaborating with similar efforts in the community, [the Internet Archive] developed a loose typology of program models for research services . . . Programming Support Model: Researchers, generally non-technical, are given time with specialized technical support staff (engineers) to collaboratively build or aggregate datasets and perform analysis.",Jefferson Bailey,
"In testing multiple models internally, and surveying and collaborating with similar efforts in the community, [the Internet Archive] developed a loose typology of program models for research services . . . Middleware Model: The creation of specific tools and platforms that operate between data hosted with a custodian and advanced analytics tools maintained externally.",Jefferson Bailey,
"In testing multiple models internally, and surveying and collaborating with similar efforts in the community, [the Internet Archive] developed a loose typology of program models for research services . . . Derivative Model: Provide pre-defined datasets that contain key extracted, derived, or pre-analyzed data culled from specific resources. The derived datasets support specific research questions, are fungible, and align data and delivery with researcher need.",Jefferson Bailey,
Service models must be self-sustaining and scale. No “grant then gone.”,Jefferson Bailey,
"Services for computational access are more successful when built on top of, or expanded from, pre-existing internal systems, processes, and infrastructure. Modular, generalized, and interoperable are preferred and boutique services don’t scale.",Jefferson Bailey,
"Continually orient towards mutually reinforcing work, be it with collaborators or researchers, and always allow for generality, in partners, technologies, and models.",Jefferson Bailey,
"... while information literacy is today a routine goal of library instruction, data work that includes enabling data discovery and retrieval, maintaining data quality, adding value, and providing for re-use lags as a topic. If the library is the laboratory of the humanities, this lag impacts how the digital collections that librarians curate are used in the humanities.",Tanya Clement,
"Conversations on data science pedagogy are needed to ensure the integration of up-to-date resources, theories, and practices in data work in a curriculum that will be geared towards inclusivity and teaching the next generation of our digital workforce about data preparation and analysis in the humanities.",Tanya Clement,
"Rigorous data work requires data “carpentry” knowledge that considers validity, reliability, and usability as well as critical literacies more generally such as data quality, authenticity, and lineage, but humanists and librarians are not traditionally trained on evaluating these aspects of data.",Tanya Clement,
"Datasets, when constructed using conventional methods of data collection and organization, run a similar risk of activating institutional power and defining “credibility,” especially when the data is procured from traditional archival sources that too often excise, anonymize and erase certain subjects, transmogrifying them in turn into (almost invisible, ghosting) “objects” and “items.”","Gabrielle Foreman, Labanya Mookerjee",
What are the implications of a lexicon and set of practices/tools that rely upon and reproduce a colonial language of power and entitlement in the digital humanities as we think collectively about best practices to “leverage computational methods and tools to treat digital library collections as data”.,"Gabrielle Foreman, Labanya Mookerjee",
"Data usability means developing data models that take into account the actions that will be performed on our data. In determining the different types of data models that we can build and implement into our collections, we must consider how humanists and social scientists effectively work with data in their research and teaching. ",Harriett Green,
Our interviews with scholars revealed that the core areas of concern for researchers included the conceptualization of collections as reusable datasets and resources for scholarly communications; the ability to break apart collections into various levels of granularity to generate diverse objects of analysis; and the need for enriched metadata. ,Harriett Green,
" ... to envision the broadest potential intervention of computationally-accessible datasets, we cannot envision that the terms “scholar” and “researcher” belong to the academic or archival communities.",Jennifer Guiliano,
... what is needed is an integrated humanities data ecosystem that recognizes ... [that] the process is as important as the product. ,Jennifer Guiliano,
"It is far easier today to analyze Twitter behavior than it is to investigate public life using public data from public institutions, such as government records, cultural heritage, and science data.","Greg Jansen, Richard Marciano ",
"Researchers need to discover, scope, ship and make reference to datasets. Though we may also move computational work across them, boundaries are an important place to define stable conditions, such as custody, provenance, security, and concise technical contracts.","Greg Jansen, Richard Marciano ",
"From code notebooks to deployment scripts that provision clusters, it becomes easier to create and share compute environments.","Greg Jansen, Richard Marciano ",
"We must confront a rightly skeptical reader, who faces increasingly high-flying visualizations and claims made from them. They are correct to demand links to the underlying evidence and methods. By providing these we enrich public understanding and trust.","Greg Jansen, Richard Marciano ",
Digitization alone is not enough to support large-scale computational analysis of library collections. Rather the more difficult steps of digital curation will be necessary to prepare our collections for appropriate reuse. Partnership may be the key.,Lisa Johnston,
The librarian favors data that is standard: forgetting enough specifics about the collection in order to produce data that references the same vocabularies and thesauri as other collection datasets. The librarian's generalization aims to support access by many different communities of practice.,Matthew Lincoln,
"The historian favors data that is rich: replete with enough specifics that they may operationalize that data in pursuit of their research goals, while forgetting anything irrelevant to those goals. The historian's generalization aims to identify guiding principles or exceptional cases within a historical context. (No two historians, of course, will agree on what that context should be.)",Matthew Lincoln,
"The machine favors data that is structured: amenable to computation because it is produced in a regularized format (whether as a documented corpus of text, a series of relational tables, a semantic graph, or a store of image files with metadata.) In a statistical learning context, the machine seeks generalizations that reduce error in a given classification task, forgetting enough to be able to perform well on new data without over-fitting to the training set.",Matthew Lincoln,
"In considering how library collections can serve as data for a variety of data ingest, transformation, analysis, replication, presentation, and circulation purposes, it may be useful to compare examples of data workflows across disciplines to identify common data ""moves"" as well as points in the data trajectory that are especially in need of library support because they are for a variety of reasons brittle.",Alan Liu,
"... a comparative study of data workflow and provenance across disciplines (including sciences, social sciences, humanities, arts) conducted using workflow modeling tools could help identify high-priority ""data moves"" (nodes in the workflow graphs) for a library-based ""always already computational"" framework.",Alan Liu,
When publishing data for others it is a balance between providing access to the data in a format that provides the least friction for adoption and use versus how knowledge organization systems work within a cultural heritage institution. ,Matthew Miller,
"What are some additional use cases that could drive shared best practices or tools for releasing cultural heritage data? Are there more advanced preprocessing that could be done to some of the common archetypical data formats found in libraries, archives and museums? And what sort of resources are required in an organization to process datasets for public consumption?",Matthew Miller,
"Simply having author or creator information, or language information, can be very helpful. My impression is that many institutions are holding onto their data tightly, with the hope of cleaning and improving it in the future. But researchers can work with imperfect data, if its limitations are discussed frankly. We can also contribute improved data back to the institution.",Miriam Posner ,
"Developers at cultural institutions may feel that learning an API is trivial, but for many people, the availability of simple flat files can be the difference between using and not using a dataset. I therefore hope that cultural institutions will consider the possibility of providing unglamorous flat files, in addition to API access to their data.",Miriam Posner ,
"Really lowbrow thought about data formats. Very simply, my students can work with CSVs, but not XML or JSON. Visualizing and analyzing the latter two formats takes programming knowledge, while even non-coders can import CSVs into Excel and create graphs and charts.",Miriam Posner ,
"Case studies. It may seem unlikely, given the recent proliferation of digital humanities journals, but it’s relatively difficult to find vetted, A-to-Z, soup-to-nuts examples of how to build visualizations and analysis from datasets.",Miriam Posner ,
"Rendering collections as datasets benefits from an understanding of the intangible and uncertain benefits of releasing collections as data and of the barriers to uptake, ideally grounded in conversations with or prototypes for potential users. Libraries not used to thinking of developers as 'users' or lacking the technical understanding to translate their work into benefits for more traditional audiences may find this challenging. ",Mia Ridge,
"Publishing data benefits from workflows that allow suitably cleaned or enhanced records to be re-ingested, and export processes that can regularly update published datasets (allowing errors to be corrected and enhancements shared), but these are all too rare.",Mia Ridge,
"No library collection is an island. Library collections are not simply a list of ones and zeros that wait to be consumed and reused, then spat out again as something different. At least, not when we want to be able to cite them. ",Hannah Skates Kettler,
"Understanding that our data are unique, it does not necessarily follow that access should be as unique and idiosyncratic.",Hannah Skates Kettler,
"When Google recently released half a million hours of video, they did it not as image stills but as vectorized features read by a neural network. These features--essentially, a computer's rough summary of an artifact into a few hundred numbers--could make it possible to researchers and students to immediately engage in computational analysis without having to wade through the preparatory steps. If done according to shared standards, they could make collections interoperable in striking ways even when texts or images can't be distributed.",Ben Schmidt,
"So for me, one of the key questions is how we expose data to facilitate the use of computational methods while preserving some of the difficulties and irregularities – the chisel marks in the smooth worked surface – that remind us of its history and humanity.",Tim Sherratt,
"I’m not sure whether this is a metadata question, or a matter of how we frame the relationship between researcher and institution. If we think of machine-actionable data as a product or service delivered by institutions, then researchers are cast as clients or consumers. But if each dataset is not a product, but a problem, then we open up new spaces for collaboration and critique.",Tim Sherratt,
"What seems to happen fairly regularly is that I find where the systems are broken. For example, while harvesting debates from the Australian parliament’s online database, I discovered about 100 sitting days were missing. This sort of thing happens with complex systems, and the staff at the Parliamentary Library have now fixed the problems. For me, it’s an example of the fact that we can never simply accept what we’re given – search interfaces lie, and datasets have holes. But it’s also shows that once you open up channels for the transmission of data, information flows both ways.",Tim Sherratt,
"We can’t talk about the need for institutions to provide computation-ready data without considering what they might get in return. The struggle for access might not always be comfortable, but it can be productive. If data is a problem to be engaged with, rather than a service to be consumed, then we can see how researchers might help institutions to see their own structures differently. On a practical level, how might we make it easier for institutions to re-ingest the features and derivative structures identified through use.",Tim Sherratt,
"I’m also a bit suspicious of scale. Big solutions aren’t always best. Large data dumps are great for researchers with adequate computing power and resources, but APIs support rapid experimentation and light-weight interventions. ",Tim Sherratt,
"... while articulating best-practice for computation-ready data we shouldn’t lose sight of other ways data can be exposed. I want hackable websites as well as downloadable CSVs – all that basic stuff like persistent urls, semantic html, and maybe a sprinkle of RDFa or JSON-LD, enables data to be discovered everywhere, not just in a designated repository.",Tim Sherratt,
"My perspective is on the very practical. Institutions have spent a lot of time, effort, and money on digitizing collections and establishing policies and infrastructures around the model of access that mimics analog models. Transforming the technology, staff, and practice to accommodate data analysis is a second paradigm shift that will be just as difficult. ",Kate Zwaard,
